\documentclass[conference]{IEEEtran}

\IEEEoverridecommandlockouts
\newcommand{\funding}{%
  This work is partially supported by the European Union under grant nos. FP7-604102 (HBP), FP7-287701 (BrainScales-Extension), and ERC-320689 (BIMPC), and by grant EP/G015740/1 (BIMPA).
}

\title{An efficient SpiNNaker implementation of the Neural~Engineering~Framework}
\author{%
  \IEEEauthorblockN{Andrew~Mundy and Jamie~Knight}
  \IEEEauthorblockA{School of Computer Science,\\
                    University of Manchester,\\
                    Oxford Road, Manchester,\\
                    M13 9PL, UK\\
                    Email: andrew.mundy@ieee.org}
  \and
  \IEEEauthorblockN{Terrence~C.~Stewart}
  \IEEEauthorblockA{Centre for Theoretical Neuroscience,\\
                    University of Waterloo,\\
                    Waterloo, ON,\\
                    Canada N2L 3G1\\
                    Email: tcstewar@uwaterloo.ca}
  \and
  \IEEEauthorblockN{Steve~Furber}
  \IEEEauthorblockA{School of Computer Science,\\
                    University of Manchester,\\
                    Oxford Road, Manchester,\\
                    M13 9PL, UK\\
                    Email: steve.furber@manchester.ac.uk}
  \thanks{\funding}
}

%% Utilities
\usepackage{booktabs}  %% Pleasant tables
\usepackage[binary-units]{siunitx}
\usepackage[caption=false, font=footnotesize]{subfig}
\usepackage{graphicx}
\usepackage[mode=buildmissing]{standalone}

%% Source highlighting
\usepackage{listings}
\lstset{
  language=Python,
  morekeywords={with,as},
  columns=flexible,
  gobble=2,
  basicstyle=\footnotesize,
  xleftmargin=1em,
}

%% Maths
\usepackage{amssymb}
\renewcommand{\vec}{\mathbf}  % Bold vectors
\newcommand{\semanticpointer}{\textbf}
\newcommand{\msemanticpointer}[1]{\mbox{\semanticpointer{#1}}}

%% Bibliographies
\usepackage[backend=bibtex, style=ieee, doi=false, url=false, maxbibnames=7]{biblatex}
\bibliography{paper}
\usepackage{xcolor}

\begin{document}
  \maketitle

  \begin{abstract}
Building and simulating neural systems is a promising avenue in our search for understanding how the brain may work and for how neural and cognitive systems may be employed to tackle engineering problems.
The Neural Engineering Framework (NEF) is an interesting hypothesis about how such systems may be constructed and has recently been used to build the world's first functional brain model, Spaun.
However, while the NEF simplifies the design of complex neural networks simulating these using standard computer hardware is still computationally expensive -- often running far slower than biological real-time and scaling very poorly: problems the SpiNNaker neuromorphic simulator was designed to solve.
In this paper we (1) argue that employing the same model of computation used for simulating general purpose spiking neural networks on SpiNNaker for NEF models is sub-optimal, and (2) provide and evaluate an alternative simulation scheme which overcomes the memory and compute challenges posed by the NEF.
The method we describe is capable of simulating up to \num{2000} neurons on a processing core and results in the order of a \SI{90}{\percent} reduction in memory used to store synaptic connection weights.
  \end{abstract}

  \section{Introduction}

For a given power budget, two factors limit the simulation of neural networks on any computing platform: scale and time. In principle any scale of network may be simulated but as scale increases simulation time follows.
Conversely, if the simulation time is limited (for example, if biological real-time is necessary) then only a limited scale of network may be simulated.
Specialised \textit{neuromorphic} hardware tries to avoid these constraints by parallelising and distributing computational effort and relying on dense interconnection of the computing elements.
The SpiNNaker platform \parencite{Furber2014} is one of a range of neuromorphic simulators (including Neurogrid \parencite{Benjamin2014}, BrainScaleS \parencite{Schemmel2010} and lately TrueNorth \parencite{Merolla2014}) which should benefit investigators of embodied cognition and researchers of large neural models alike by enabling scalable, rapid simulation of large-scale neural networks.

The Neural Engineering Framework (NEF) \parencite{Eliasmith2004} is a hypothesis about how neurons may act to encode the abstract mathematical constructs, such as scalars and vectors, that we often use in modelling the real world.
Its successes so far include the Spaun model of cognition \parencite{Eliasmith2012}.
As with all neural systems, the NEF has proven costly to simulate: the Spaun model typically required \SI{2.5}{\hour} of compute time for \SI{1}{\second} of simulation \parencite[\S V]{Stewart2014}.
Two aspects of NEF networks that make them particularly costly to simulate are the high firing rates of individual neurons (around \SIrange{200}{400}{\hertz} when saturated) and the dense synaptic matrices used to connect neuronal populations.
The former presents a significant communication cost to any specialised neuromorphic hardware and the latter requires that large amounts of memory be used to represent the neural network with all the associated costs of transferring large blocks of data that implies.

  In this paper we:
  \begin{enumerate}
    \item argue that due to these properties of the NEF, the existing solutions and algorithms used to simulate neural networks on SpiNNaker will not scale satisfactorily to Spaun-like models,
    \item detail a method by which features of the NEF may be used to reduce the memory and compute costs associated with its simulation.
  \end{enumerate}

We hope to use the result to run the full Spaun model in biological real-time -- a \num{9000} times speedup.

  \section{Background}

In this section we briefly discuss the SpiNNaker platform and how neural networks are currently simulated on it before introducing the Neural Engineering Framework (NEF) and discussing how models built with it may act to stress SpiNNaker in various ways.

  \subsection{The SpiNNaker platform}

The SpiNNaker platform is a massively parallel architecture designed to simulate neural networks.
A SpiNNaker machine is constructed from a number of SpiNNaker chips, each connected to their six immediate neighbours using a chip-level interconnection network with a toroidal, triangular mesh topology.
Each SpiNNaker chip contains 18 ARM processing cores connected, via a network-on-chip, to each other and the external network through a multicast router.
Each core has two small tightly-coupled memories: \SI{32}{\kibi\byte} for instructions~(ITCM) and \SI{64}{\kibi\byte} for data~(DTCM) and shares \SI{128}{\mebi\byte} of off-chip SDRAM with the other cores on the SpiNNaker chip.

SpiNNaker is an event-driven message-passing computing architecture.
The software running on a core may transmit packets to other processing cores to indicate the occurrence of events or to share data.
A packet consists of a \SI{32}{\bit} key -- which is used to direct the packet around the network and often indicates something of the nature of the event or attached data -- and, optionally, a \SI{32}{\bit} data payload.
When a packet reaches a router the key is inspected to determine to which (if any) of the 18 processors and six external links attached to the router it should be forwarded.
On receipt of a packet a core executes a \textit{callback} function which may inspect the packet and schedule further execution as required.

  \subsection{Simulating neural nets on SpiNNaker}
  \label{sef:background/nn}

When simulating neural nets on a SpiNNaker machine each core is responsible for simulating a number (in the order of a few hundred) of point neurons.
When one of these neuron spikes it transmits a packet whose key uniquely identifies the neuron (for this it requires no payload).
This ``spike'' packet is then routed across the network fabric to the processing cores responsible for simulating each of the neurons that are synaptically connected to the firing neuron.
On receipt of a ``spike'' packet a core retrieves the row of the connectivity matrix associated with the firing neuron from SDRAM.
Each of these rows describes the synaptic weights and delays associated with the connections between the firing neuron and those simulated on the core.
Once a row is retrieved the weights are inserted into an input ring-buffer where they remain until the synaptic delay has elapsed when they are used to calculate the neuronal input current.

There are two primary constraints to the number of neurons that may be simulated on a single processing core:

  \begin{enumerate}
    \item The amount of memory required to store the synaptic weight matrices must fit within the space available to the core.
    \item The majority of processing time is spent in the synaptic processing pipeline, so there must be sufficient time for the core to process all incoming `spike' packets; and retrieve and process the synaptic rows during one simulation time-step (see \parencite{Sharp2013}).
  \end{enumerate}

These constraints may be met by either allocating fewer neurons to each processing core or by increasing the processing time used for each simulation time-step.

In practice there are limits to the number of processors and the amount of time that are available (though one aim of the SpiNNaker project is to build a machine consisting of one million cores).
Hard time constraints are necessary when SpiNNaker is required to run in biological real-time, as it is in experiments with other neuromorphic hardware.
Processor constraints are present for those with access to only small SpiNNaker machines.
At some point it is necessary to ensure that efficient use is made of the machine with regard to both time and processor usage.

Each core is allocated $\frac{1}{16}$ of the \SI{128}{\mebi\byte} SDRAM and hence the synaptic matrices must fit within \SI{8}{\mebi\byte} of memory -- this forms the first constraint.
The second constraint is related to both the number of spikes received per time-step and the density of the synaptic matrix: \textcite[\S III.C]{Sharp2013} indicate that at most there may be \num{5000} synaptic events per millisecond when running at biological real-time.
A single synaptic event indicates one spike being passed through a single synapse to a neuron on the receiving core.

It should be noted that time is also a factor \textit{prior} to the start of any simulation.
All data required by the SpiNNaker machine during simulation must be transmitted to it across ethernet, consequently the more data required on the machine the greater the time required to prepare it for simulation.
\textcite{Sharp2013} note that this can take a significant period of time and that this is undesirable if a real-time simulator is desired.
Reducing the memory usage is important to ensure both that efficient use is made of processing cores but also that the time required to load data to a SpiNNaker machine is reasonable.

  \subsection{The Neural Engineering Framework}
  \label{sef:background/nef}

The Neural Engineering Framework (NEF) extends the concept of ``preferred-direction vectors'' \parencite{} to all neural populations.
Each population represents a vector within a particular space (typically a hypersphere) and within that population, each the firing rate of each neuron reflects the similarity of the represented vector to the ``encoding'' vector for the neuron.
Using the notation of \textcite{Stewart2014} this may be expressed as:

\begin{equation}
  \delta_{i}\left(\vec{x}\right) = G_{i}\left[ \alpha_i \vec{e}_i \cdot \vec{x} + J^{bias}_i \right]
  \label{eq:encoding}
\end{equation}

Which states that the firing response of neuron $i$ ($\delta_i$) to the represented value ($\vec{x}$) is the response of the neuron model ($G_{i}$) to an input consisting of a randomly selected gain term ($\alpha_i$), the encoding vector for the neuron ($\vec{e}_i$) and a fixed bias current ($J^{bias}_i$).
The process of ``encoding'' allows us to transform a variable in vector form into the spiking response of a population of neurons.
Correspondingly, ``decoding'' allows a transformation from the spiking actions of neurons into the domain of vectors.
Again using the notation of \textcite{Stewart2014} we can express this decoding process as:

\begin{equation}
  \vec{\hat{x}} = \sum\limits_{i=1}^{N} a_i(\vec{x})\vec{d}_i  \label{eq:decoding}
\end{equation}

Where the estimate of the original represented value ($\vec{\hat{x}}$) is the sum of the spiking activity of each neuron ($a_i$) multiplied by the linear decoder for the neuron ($\vec{d}_i$).
The decoding vectors may be selected to compute a function of the value represented by the population.
\figurename~\ref{fig:background/nef-1} illustrates the encoding of a two-dimensional value using four neurons -- the role of the encoding vectors can be seen in that each neuron becomes active for only a small range of the input space.

  \begin{figure}[!t]
    \includegraphics{figures/nef-1}
    \caption{Representing a 2-dimensional value using four neurons. The input values and spiking responses of the neurons are shown in the top plot. The bottom left-hand plot shows how the firing responses vary to the angle of the 2-D input vector (tuning curves) and the bottom right-hand plot shows a decoding of the population's representation along with the input value.}
    \label{fig:background/nef-1}
  \end{figure}

%% Synaptic filtering
For a connection between a pair of populations a (dense) synaptic weight matrix can be calculated by computing the matrix product of the decoders of the pre-synaptic population and the encoders of the post-synaptic population \parencite{Stewart2014}:

\begin{equation}
  \omega_{ij} = \alpha_j \vec{d_i}\vec{e_j}  \label{eq:weights}
\end{equation}

With $i$ indexing neurons in the pre-synaptic population and $j$ those in the post-synaptic population.

  \subsection{Assessing the Neural Engineering Framework (NEF)}
  \label{sef:background/assessing}

As discussed in \S\ref{sef:background/nef}, it is entirely possible to translate models built using NEF principles into populations of point neurons connected with dense synaptic weight matrices.
These networks could then, in theory, be simulated using the standard SpiNNaker spiking neural network simulator described in \S\ref{sef:background/nn}.
In this section we show that, based on parameters used in the Spaun functional brain model, this approach would be sub-optimal given the constraints of the SpiNNaker platform outlined in \S\ref{sef:background/nn}.

The Spaun model is built using the Semantic Pointer Architecture (SPA) \parencite{eliasmith2013build} which uses randomly chosen NEF unit vectors to represent basic concepts (\semanticpointer{MOUSE}, \semanticpointer{CHEESE}, \semanticpointer{SUBJECT}, \semanticpointer{OBJECT}, \semanticpointer{EAT}, \semanticpointer{VERB}).
These concepts can then be combined into symbol structures using addition~($+$) and circular convolution~($\circledast$) operators, both functions being performed which are evaluated using the NEF.
The circular-convolution operator is non-linear and, Eliasmith \parencite{eliasmith2013build} demonstrates, require around 70 neurons per-dimension to calculate accurately.

For example a semantic pointer representing the sentence ``Mice eat cheese'' can be calculated as $\msemanticpointer{S} = \msemanticpointer{MOUSE} \circledast \msemanticpointer{SUBJECT} + \msemanticpointer{EAT} \circledast \msemanticpointer{VERB} + \msemanticpointer{CHEESE} \circledast \msemanticpointer{OBJECT}$.
The subject of this sentence could then be extracted using $\msemanticpointer{S} \circledast \msemanticpointer{SUBJECT}^{-1} \approx \msemanticpointer{MOUSE}$.
However, this later process is relatively noisy -- to extract the $\msemanticpointer{MOUSE}$ semantic pointer the result needs to be ``cleaned up''.
This process is performed using an auto-associative memory network \parencite{Stewart2011} the properties of which dictate that, in order to reliably extract a human-scale lexicon from a semantic pointer, it requires around 500 dimensions.

On the basis of these two parameters (500 dimensions and 70 neurons per dimension) we can determine that functions on these vectors must be evaluated using $3.5\times10^4$ neuron populations -- two of which would be connected using a dense synaptic matrix with $1.225\times10^9$ entries.
Synapses on SpiNNaker are typically represented as \SI{32}{\bit} values, hence the synaptic matrix would be approximately \SI{4.5}{\gibi\byte}.
As each core only has \SI{8}{\mebi\byte} of SDRAM each of these populations would have to be distributed between 585 cores, reducing the number of neurons simulated per core to only 53 -- several orders of magnitude short of SpiNNaker's architectural target of 1000 \parencite{}. 

Representational errors can be reduced both by increasing the number of neurons and allowing these to fire at a higher rate \parencite{Eliasmith2004}. 
NEF models such as Spaun typically opt to use fewer neurons firing at rates of up to \SI{400}{\hertz}. 
Consequently {\color{red} when using typical tuning curves, neurons fire at an average rate of approximately \SI{100}{\hertz}}. 
\figurename~\ref{fig:results/network-utilisation} shows that, if 70 neurons are used per-dimension, a 4-D ensemble will receive around 20 incoming spikes per time-step.
Due to the dense connectivity matrices used by NEF each of these spikes would result in 280 synaptic events -- exceeding the constraint of 5000 synaptic events per time-step.

  \begin{figure}[!t]
    \centering
    \includegraphics[scale=0.8]{figures/network-1}
    \caption{Spike rates for the pre-synaptic population of a communication channel.  The vertical line indicates the point at which \num{5000} synaptic events per core has been passed. Each population had 70 neurons per dimension.}
    \label{fig:results/network-utilisation}
  \end{figure}

  \subsection{Constructing models with the NEF}
\label{sef:background/constructing}
Neural models built using the principals of the NEF may be implemented in any programming language, however, a standard Python library for building and simulating these models has been developed. Nengo \parencite{Bekolay2014} provides several core components which may be used to specify models for simulation:

  \begin{description}[\IEEEsetlabelwidth{Connections}]
    \item[Ensembles] A population of neurons with their attendant encoders.
    \item[Nodes] General purpose non-neural components, typically used to inject values into simulations or to drive real-world actuators.
    \item[Connections] Specify how nodes and ensembles are connected.
    \item[Probes] Allow gathering of simulation data for later analysis.
  \end{description}

  An illustrative model that we will use later in this paper is the \textit{communication channel}. A communication channel consists of two ensembles connected with the synaptic weights chosen such that the second ensemble will represent the same value as the first ensemble. The concept is illustrated in \figurename~\ref{fig:background/comms-channel}. Using Nengo a communication channel may be constructed as:

  \begin{lstlisting}
  with nengo.Network('Communication Channel') as comms_channel:
      # 2 100 neuron ensembles, each of 2-dimensions
      ens_a = nengo.Ensemble(100, 2)
      ens_b = nengo.Ensemble(100, 2)

      # Connection from A to B with the identity function
      nengo.Connection(ens_a, ens_b)
  \end{lstlisting}

  \begin{figure}
    \centering
    \includestandalone[scale=0.75]{figures/comms_channel_map}
    \caption{A NEF communication channel consisting of two populations of neurons and a connection.
    The diagram indicates how the abstract communication channel might be instantiated on a simplified representation of the SpiNNaker architecture:
    Population $A$ is split between processing two cores, consequently two streams of multicast packets allow simulation of the communication channel.
    }
    \label{fig:background/comms-channel}
  \end{figure}

  \section{Exploiting features of the NEF for effective simulation on SpiNNaker}
  \label{sec:exploiting}
  In this section we review an alternative simulation scheme that will meet the constraints of the SpiNNaker hardware for a large range of networks constructed using the NEF.
  % The NEF permits us two different views of the activity of a neural population: that of spikes and that of a set of time-varying vectors.
  % In our simulation scheme we use the second of these to facilitate communication of neural activity between processing cores.
  The details of the NEF described above suggest this alternate, more efficient, implementation for SpiNNaker.
  We note that the synaptic weight matrices are computed via (\ref{eq:weights}).
  This means that all the synaptic weight matrices are exactly \textit{factorable}.
  Moreover every set of connection weights coming into an ensemble will have the same $\alpha_j \vec{e}_j$ factor applied to it.
  We can thus \textit{split} the synaptic connection weight matrices into two parts: the ``encoder'' $\alpha_j \vec{e}_j$ and the ``decoder'' $\vec{d}_i$.
  These matrices are of size $N \times D$ and $D \times N$, where $D$ is the ``dimensionality'' of the representation and is generally much smaller than $N$, the number of neurons.
  We can store the encoder in the core that is simulating the post-synaptic population, and we can store the decoder with the pre-synaptic population.
  This saves significant memory since the two factors are much smaller than the original weight matrix and we only need to store one encoder no matter how many sets of input connections there are.
  However, since the connection weight matrix is split in this way, we can no longer send a packet to represent each spike.
  Rather, the spikes are multiplied by the decoder and the resulting vector value is transmitted.
  Importantly, since the original weight matrices are perfectly factorable, the result is identical to the original SpiNNaker approach in terms of the neural behaviour, but with significantly reduced memory requirements, and, as we shall show, improved processor utilisation.
  \figurename~\ref{fig:algorithm/spikes} illustrates the scheme with the full weight matrix stored in the memory of the post-synaptic core and \figurename~\ref{fig:algorithm/values} illustrates the proposed scheme with the factored matrix split across the pre- and post-synaptic cores and with ``value''-packets being transmitted between the processors.

  \begin{figure*}
    \subfloat[Spike-based transmission]{%
      \label{fig:algorithm/spikes}
      \includegraphics[width=\textwidth, page=2]{figures/algorithm_diagram}}\\
    \subfloat[Value-based transmission]{%
      \label{fig:algorithm/values}
      \includegraphics[width=\textwidth, page=1]{figures/algorithm_diagram}}
      \caption{A comparison of the (a) spike-based and the (b) proposed value-based algorithms.}
  \end{figure*}


  We split the execution time of a processing core into three steps: \textit{input filtering}, \textit{neuron update} and \textit{output}.
  In the ``spike'' simulation scheme the ``inputs'' to a processing core were in the form of ``spike'' packets which were used to drive a pipeline which retrieved synaptic weights from SDRAM; in our proposed scheme the inputs take the form of a set of packets whose payloads can be combined to form a vector.
  For example, the processing core simulating population $B$ from \figurename~\ref{fig:background/comms-channel} will receive two packets per time-step from each of the two cores simulating population $A$ -- combining these packets will form a 2-D vector representing the current decoding of the activity of $A$.
  At the start of each simulation step the reconstructed vector is filtered using the appropriate synaptic filter model to form the input to the population -- this is the \textit{input filtering} step.

  During the \textit{neuron update} step the state of each neuron is updated in turn.
  First the input for the neuron is computed by calculating the dot product between the encoding vector for the neuron and the current input to the population ($\alpha_i \vec{e}_i \cdot \vec{x} + J^{bias}_i$ from (\ref{eq:encoding})).
  Then the neuron itself is simulated, for example using the Euler method to compute the next state of a LIF neuron.
  If, during this process, a neuron spikes then its decoding vector ($\vec{d}_i$) is looked up and added to a buffer containing the current decoding of the population activity (as in (\ref{eq:decoding})).
  Once all the neurons have been simulated the output buffer contains a vector which represents the weighted decoding of any spikes which occurred during the simulation step.

  During the \textit{output} stage each element of the output buffer vector is transmitted in the payload of a multicast packet with a key which uniquely identifies the population, decoding vector and element index.
  The communication fabric routes these ``value'' packets to cores which simulate connected populations of neurons.

  Some ensembles may have multiple outgoing connections with different transformations or functions, as a result there are multiple decoder matrices which need to be applied when decoding the activity of the population.
  In these cases we combine the matrices such that the decoding vector for any neuron is the concatenation of the required decoders
  (i.e., $\vec{d}_i = \left[\vec{d}_i^{f(\vec{x})},\,\vec{d}_i^{g(\vec{x})},\,\ldots \right]$).

  \section{Results}
  \label{sec:results}

  \subsection{Processor utilisation}
In order to measure the CPU usage incurred when running the algorithm described in \S\ref{sec:exploiting} on SpiNNaker, we developed a simple tool to profile SpiNNaker executables by manually defining profiling regions within them.

In this section, we use this profiling tool to analyse how the CPU usage of the ensembles that make up the communication channel network described in \S\ref{sef:background/constructing} vary with the number of dimensions they are representing and the number of neurons they use to do so. 
\figurename~\ref{fig:results/comm-channel-cpu} show how the CPU time spent in the different phases of the algorithm outlined in \S\ref{sec:exploiting} increases with the number of dimensions and neurons. 
As a packet is sent and received every time-step for each dimension, the CPU time spent in both the \textit{input filtering} and \textit{output} phases is only dependant on the dimensionality $D$. On this basis, we fitted the following simple models of CPU time spent in these phases to the profiling data by performing a minimisation of the mean-squared error:
%
\begin{eqnarray}
  t_{input} & = & 245 + 43 \cdot D \label{eq:profiling_input}\\
  t_{output} & = & 100 + 702 \cdot D \label{eq:profiling_output}
\end{eqnarray}
%
As each neuron needs to apply its encoder vector to the filtered input and, if it spikes, add its decoder vector to the output buffer, the time spent in the \textit{neuron update} phase is dependant both on the dimensionality as well as the number of neurons $N$. To model this, we again fitted a simple, 1st order, 2-D model to the profiling data by performing a minimisation of the mean-squared error:
%
\begin{equation}
  t_{neuron} = 188 + 69 \cdot N + 13 \cdot N \cdot D\label{eq:profiling_neuron}
\end{equation}
%
By adding these equations together, we can model the total CPU usage as follows:
%
\begin{equation}
  t = 533 + 745 \cdot D + 69 \cdot N + 13 \cdot N \cdot D\label{eq:profiling_total}
\end{equation}
%
The 69 cycle cost of updating each neuron is a significant improvement over the 128 cycles quoted by \textcite{Sharp2013}. 
The reason for this is likely to be due to the the Nengo neuron models use of normalised membrane voltages and having many parameters common to an entire population of neurons. 
This means that fewer memory instructions are required to transfer each neurons state into the register file when compared to the general purpose SpiNNaker simulator. 

In order to not overload the network interconnect, the simulator waits for \SI{1}{\micro\second} between sending packets in the \textit{output} phase, resulting in the high cost per-dimension shown in (\ref{eq:profiling_output}). 
The remaining terms of (\ref{eq:profiling_total}) are then approximately synonymous with the synaptic processing stage of the general purpose SpiNNaker simulator analysed in \S\ref{sef:background/assessing}.
Here the CPU benefits of the new simulation scheme become clear as, when we fit a similar model to the spike-based results shown in \figurename~\ref{fig:results/network-utilisation} we obtain the following equation:
%
\begin{equation}
  t = 3 \cdot N^{2}
\end{equation}
%
Which, as $N \gg D$, grows much more rapidly than (\ref{eq:profiling_total}).

  \begin{figure*}[!t]
    \centering
    \subfloat[16-D]{
      \includegraphics[width=2.5in]{figures/comm_channel_cpu_16d_bar}
    }
    \hfil
    \subfloat[100 neurons per-population]{
      \includegraphics[width=2.5in]{figures/comm_channel_cpu_100n_bar}
    }
    \caption{Average percentange of 1ms simulation time step spent in different phases of our simulation algorithm when simulating communication channels of varying dimensionality using different numbers of neurons.}
    \label{fig:results/comm-channel-cpu}
  \end{figure*}

  \subsection{Memory utilisation}

  The dense synaptic weight matrix for any connection between populations of neurons in the NEF will be of size $N_\mathrm{pre} \times N_\mathrm{post}$ and, under the spike based algorithm, would be stored entirely in the memory of the post-synaptic core.
  When factored weight matrices are substituted for the full weight matrix, as described in \S\ref{sec:exploiting}, then the memory usage is split between the pre- and post-synaptic cores.
  The post-synaptic core needs to store a single encoder matrix of size $D_\mathrm{post} N_\mathrm{post}$ while the pre-synaptic core will store a decoder matrix of size $D_\mathrm{post} N_\mathrm{pre}$.

  As an example, in the case of a 16-D communication channel with \num{1120} pre-synaptic neurons and the same number of post-synaptic neurons the full weight matrix will require \num{1254400} values to be stored in the memory of the post-synaptic core.
  If factored weight matrices are used then the pre-synaptic core must store \num{17920} values and the post-synaptic core the same.
  The post-synaptic core will not need to store any more data for any further incoming connections.
  Using, as is common, \SI{32}{\bit} to store each value this becomes \SI{4.79}{\mebi\byte} to store the full weight matrix or \SI{70.00}{\kibi\byte} to store the encoder and the same for the decoder (a total of \SI{140.00}{\kibi\byte}).
  Factored weight matrices achieve a saving~of~\SI{97.14}{\percent}.

  \begin{table}
    \centering
    \caption{Memory Usage of Synaptic Weights in Basal Ganglia Model}  %% Yuck, Title Case
    \label{tab:results/memory-bg70}
    \input{figures/bgscale_70}
    \vskip\baselineskip

    \raggedright Memory usage of the synaptic weights (full matrix and factored) for the standard Basal Ganglia implementation from Nengo (70 neurons per dimension).
  \end{table}

  Table~\ref{tab:results/memory-bg70} shows the memory usage from the connections within the basal ganglia model used in the Spaun model.
  The reduction in memory usage that results from using factored weight matrices is significant -- it should be noted that for a basal ganglia accepting \num{128}-D inputs representing the connection weights would require the SDRAM of nearly five SpiNNaker chips if full weight matrices were used.

  \subsection{Simulating large-scale models}
  \label{sec:spa-sequence}

In this section we evaluate the performance of our new simulation scheme by simulating a larger NEF network based around the model of action selection developed by \textcite{Stewart2010}. 
In our benchmark, we configure the Basal Ganglia portion of this model to select from a list of semantic pointers and then, based on this, use Thalamus to emit the ``next'' semantic pointer in the list. By recurrently connecting the output of the Thalamus to the input of the Basal Ganglia, our network then iterates through the list of semantic pointers as shown in \figurename~\ref{fig:results/sequence}.

To demonstrate the run-time savings made possible with our new scheme, we simulated the network for 10s at two different scales using instrumented versions of both the new SpiNNaker simulator and the Nengo reference simulator \parencite{Bekolay2014} running on a standard desktop PC (\SI{3}{\giga\hertz} AMD Athlon II X3 445). 
The results of this process, shown in \figurename~\ref{fig:results/spa-wall-clock}, illustrate that while  performing the processing required to map the problem to SpiNNaker and transferring data to and from the SpiNNaker board, adds to the simulation time, this is outweighed by the actual simulation time remaining constant, even for relatively small networks.
 
  \begin{figure}[!t]
    \includegraphics{figures/sequence}
    \caption{Comparing the output of the action selection against a list of 4 semantic pointers, labelled $A$, $B$, $C$ and $D$ - showing sequence behaviour.}
    \label{fig:results/sequence}
  \end{figure}

  \begin{figure}[!t]
    \includegraphics{figures/spa_wall_clock}
    \caption{}
    \label{fig:results/spa-wall-clock}
  \end{figure}
  \section{Discussion}

  \subsection{Comparison to prior SpiNNaker implementations}

  \textcite{Galluppi2012} demonstrated a mapping from models written in Nengo to the SpiNNaker architecture using the older simulation method described in \S\ref{sef:background/nn}.
  This implementation also used the encoding/decoding processes to reduce communication overheads between computational elements, but, unlike the method presented here, only applied it to communication between SpiNNaker and an external computer.
  Instead, use of spike-based transmission necessitated the storage of complete weight matrices and led to a significant amount of time being required to transfer simulation data to the SpiNNaker machine, \citeauthor{Galluppi2012}\ suggested that this may be overcome by transmitting encoders and decoders separately and performing the matrix multiplication on SpiNNaker.
  This would have reduced the load time but would not have solved the problem of the synaptic matrices filling the usable memory, nor the associated issues of spike rates and network usage.
  For the reasons advanced above we believe that this implementation of the NEF on SpiNNaker would not have satisfactorily scaled to simulate models of the scale or complexity of Spaun.

  \subsection{Comparison to other neuromorphic simulators}

  SpiNNaker is but one of a range of neuromorphic simulators, a family including both analogue (e.g., Neurogrid \parencite{Benjamin2014}, BrainScaleS \parencite{Schemmel2010}) and digital (e.g., TrueNorth \parencite{Merolla2014}) hardware, which aim to reduce the power consumption and execution time for simulating large neural models.
  Of these, the analogue simulators can be expected to require less power than SpiNNaker for a given scale of model \parencite{Stromatias2013} and to be able simulate faster than biological real-time; however, they are limited to simulating neurons and synapses of the type cast in silicon whereas SpiNNaker may be reprogrammed as required to investigate new models and learning rules.

  \subsection{Comparison to general purpose computer simulators}

  \S\ref{sec:spa-sequence} illustrated how the SpiNNaker compared to the Python implementation of Nengo when simulating models of a communication channel and a more complicated example highlighting features of the NEF and SPA.
  For some scales of models it may be expected that the overheads of specialised hardware (including SpiNNaker) are sufficiently great that simulation on standard computing hardware and GPUs can be expected to be faster.
  When this is the case, the Python implementation of Nengo of \textcite[\S6]{Bekolay2014} is shown to compare favourably with several standard neural simulators (Brian \parencite{Goodman2009}, NEURON \parencite{carnevale2006neuron} and NEST \parencite{Gewaltig2007}) on a range of networks, and an OpenCL implementation of Nengo is shown to perform even better but is only demonstrated on feed-forward networks.
  However, when simulating models on the scale of Spaun we expect the simulation scheme detailed in this paper to provide substantial speed improvements over standard computing hardware even when the overheads are taken into account; moreover as SpiNNaker is a real-time simulator we expect it to perform well in conjunction with neuromorphic sensors or actuators.

  \subsection{General applicability of the algorithm}

  SpiNNaker is a example of a message passing architecture -- consequently we expect that our simulation scheme, transmitting decodings of ensemble activity rather than spike activity, to be equally applicable to simulating neural nets on any message passing distributed architecture.
  Despite this the particular advantages of SpiNNaker (low-power and real-time) suggest that it is a good choice of platform on which to simulate large-scale neural models.

  \subsection{Future work}

  This work has laid the basis for simulating truly large-scale models built using the NEF in biological real-time.
  Several improvements are planned which should enable a greater number of neurons to fit on a single core and we anticipate the implementation of further neuron and synapse models.
  In particular, our current implementation only allows for one encoder per neuron and this is likely to be limiting in light of learning rules (e.g., \parencite{Voelker2014}) which act to modify weights in encoders.
  We also intend to better assess the computational cost or gain of value-based transmission on implementing learning rules and a constant traffic pattern affects the SpiNNaker network architecture.

  \section{Conclusion}

  This paper has shown how several properties of the NEF lead to inefficient use of the SpiNNaker computing architecture.
  We have described and assessed an alternative approach to simulating models built using the NEF on SpiNNaker that enable it to exceed the design target of 1000 neurons per core without exceeding the network capacity or becoming memory bound.
  Our intent is to use this new implementation of the NEF for SpiNNaker to simulate the full Spaun-model in biological real-time, and to investigate models of greater scale and complexity.

  \section*{Acknowledgements}

The authors would like to extend their thanks to the organisers of the Telluride Neuromorphic Cognition Engineering Workshop.

  \printbibliography

\end{document}
