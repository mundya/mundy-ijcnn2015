\documentclass[handout,t]{beamer}
\usetheme[cabin, darktitle]{UniversityOfManchester}

%% Document properties
\title{An efficient SpiNNaker implementation of the Neural~Engineering~Framework}
\author{Andrew Mundy, James Knight,\\Terry Stewart and Steve Furber}

\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[binary-units]{siunitx}
\usepackage[backend=bibtex, doi=false, url=false,
            autocite=footnote, maxbibnames=3, style=authoryear]{biblatex}
\bibliography{paper}

\renewcommand{\vec}{\mathbf}
\newcommand{\semanticpointer}{\textbf}
\newcommand{\msemanticpointer}[1]{\mbox{\semanticpointer{#1}}}

\begin{document}
  \maketitle

  %% Constraints on SpiNNaker programs
  %% - Memory usage
  %%  - In its own right
  %%  - Time to load data
  %% - Processing time
  %%  - Dominated by synaptic weight lookup -- < 5000 synaptic events per timestep
  \begin{frame}{Simulating neural nets on SpiNNaker}
    \vfill
    \hspace*{-.15\textwidth}\includegraphics[page=2, width=1.3\textwidth]{algorithm_diagram}
    \vfill
    \pause
    Constraints on number of neurons per core:
    \begin{description}
      \item[Memory] Size of weight matrix
      \item[Compute] Number of synaptic events
    \end{description}
  \end{frame}

  \begin{frame}{Performance of the SpiNNaker architecture}
    Constraints:
    \begin{itemize}
      \item \SI{8}{\mebi\byte} limit on weight matrices.
      \item Max \num{5000} synaptic events per millisecond \parencite{Sharp2013}.
      \begin{itemize}
        \item One synaptic event is one spike for one synapse.
      \end{itemize}
    \end{itemize}

    \pause
    Overcome by:
    \begin{itemize}[<+->]
      \item Allocating fewer neurons to a core
        \begin{itemize}
          \item \textbf{but} power and number of processors limited.
        \end{itemize}
      \item Using longer simulation time-steps
        \begin{itemize}
          \item \textbf{but} sometimes hard deadlines.
        \end{itemize}
    \end{itemize}
  \end{frame}

  \begin{frame}{Bad characteristics}
    Fine for the majority of the cases, not helped by:
    \begin{itemize}
      \item Dense weight matrices
      \item High firing rates
    \end{itemize}
    \pause
    Both characteristics of models built with the Neural~Engineering~Framework~(NEF)
  \end{frame}

  %% Introduction to the NEF
  \begin{frame}{The Neural Engineering Framework (NEF)}
    \begin{itemize}
      \item Populations of neurons represent abstract values.
      \item Connections compute functions of representations.
      \item Extended by the Semantic Pointer Architecture (SPA).
        \begin{itemize}
          \item Used to build Spaun, ``the world's largest functional brain model''
        \end{itemize}
    \end{itemize}
  \end{frame}

  %% - Encoding
  %% - Decoding
  %% - Forming weight matrices
  %% TODO Motivate encoding, explain decoding
  \begin{frame}[plain]{Representing scalars}
    \vfill
    \includegraphics[width=\textwidth]{encoding_decoding}
    \vfill
  \end{frame}

  \begin{frame}[plain]{Representing vectors}
    \vfill
    \includegraphics[width=\textwidth]{encoding_decoding2d}
    \vfill
  \end{frame}

  \begin{frame}{Combining encoding and decoding}
    \includegraphics[width=\textwidth]{weight_matrices}

    \begin{itemize}
      \item Synaptic weights show the degree of similarity: $\omega_{ij} = \vec{d}_i \cdot \vec{e}_j$
    \end{itemize}
  \end{frame}

  \begin{frame}{The Semantic Pointer Architecture (SPA)}
    \begin{itemize}
      \item Represent ``concepts'' using high-dimensional vectors.
    \end{itemize}

    \begin{center}
      \begin{tabular}{c l}
        \toprule
          $\msemanticpointer{A} + \msemanticpointer{B}$ & A vector similar to both \semanticpointer{A} and \semanticpointer{B}. \\
          $\msemanticpointer{A} \circledast \msemanticpointer{B}$ & A vector dissimilar to both \semanticpointer{A} and \semanticpointer{B}. \\
          $\msemanticpointer{A}^\prime$ & Approximate inverse of \semanticpointer{A}. \\
        \bottomrule
      \end{tabular}
    \end{center}

    Operations computable with the principles of the NEF.
  \end{frame}

  \begin{frame}{The Semantic Pointer Architecture (SPA)}
    Represent a series of shapes with:
    \begin{align*}
      \msemanticpointer{X} &= \msemanticpointer{BLUE}\circledast\msemanticpointer{TRIANGLE} \\
                           &+ \msemanticpointer{RED}\circledast\msemanticpointer{SQUARE}\\
                           &+ \msemanticpointer{GREEN}\circledast\msemanticpointer{CIRCLE}
    \end{align*}

    Query with inverse: ``What shape is red?''
    \begin{align*}
      \msemanticpointer{X} \circledast \msemanticpointer{RED}^\prime &\approx \msemanticpointer{SQUARE}
    \end{align*}
    Result needs ``cleaning up''.
  \end{frame}

  \begin{frame}{Requirements of the NEF}
    General requirements:
    \begin{itemize}
      \item Weight matrices are dense
      \item High firing rates (typically upto \SI{400}{\hertz})
    \end{itemize}

    Heuristics for SPA:
    \begin{itemize}
      \item 70 neurons per dimension
      \item 500 dimensions for a human size vocabulary
    \end{itemize}
  \end{frame}

  %% How does the NEF meet these constraints?
  %% - Consider communication channel using Spaun-like parameters of 16-D and 70 neurons per dimension
  %%  - Memory use is insane (down to 53 neurons per core)
  %%  - Synaptic events very quickly pass the max allowed
  %% Result: using the standard approach with the NEF will lead to suboptimal use of the architecture.

  %% An alternative: value-based transmission
  %% - Note that weight matrices can be factored into encoders and decoders
  %% - Indicate how this can be used in simulation using the payloads of MC packets
  \begin{frame}{Value Transmission}
    \vfill
    \hspace*{-.15\textwidth}\includegraphics[page=1, width=1.3\textwidth]{algorithm_diagram}
    \vfill
    \pause
    \begin{itemize}
      \item Factored weight matrices $\ll$ full weight matrix
      \item Different compute requirements
    \end{itemize}
  \end{frame}

  %% Results
  %% - Memory usage
  %% - Processor usage

  %% Discussion (brief)

  %% Thanks!
\end{document}
